{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a la Ciencia de Datos: Tarea 2\n",
    "\n",
    "Este notebook contiene el código de base para realizar la Tarea 2 del curso. Puede copiarlo en su propio repositorio y trabajar sobre el mismo.\n",
    "Las **instrucciones para ejecutar el notebook** están en la [página inicial del repositorio](https://gitlab.fing.edu.uy/maestria-cdaa/intro-cd/).\n",
    "\n",
    "**Se espera que no sea necesario revisar el código para corregir la tarea**, ya que todos los resultados y análisis relevantes deberían estar en el **informe en formato PDF**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar dependencias\n",
    "Para esta tarea, se han agregado algunos requerimientos, asegúrese de instalarlos (puede usar el mismo entorno virtual de la Tarea 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in c:\\users\\santi\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\santi\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: sqlalchemy<2.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (1.4.39)\n",
      "Requirement already satisfied: pymysql in c:\\users\\santi\\anaconda3\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\santi\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\santi\\anaconda3\\lib\\site-packages (9.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\santi\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (7.6.5)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (6.19.2)\n",
      "Requirement already satisfied: jupyter-console in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (6.6.2)\n",
      "Requirement already satisfied: notebook in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (6.5.2)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (6.5.4)\n",
      "Requirement already satisfied: qtconsole in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter) (5.4.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from sqlalchemy<2.0) (2.0.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from seaborn) (3.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (8.10.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (23.2.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (7.3.4)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (1.5.6)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (5.7.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipykernel->jupyter) (0.1.6)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (3.5.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipywidgets->jupyter) (5.7.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter) (5.2.0)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.30 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter) (3.0.36)\n",
      "Requirement already satisfied: pygments in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-console->jupyter) (2.11.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.8.4)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (1.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (2.1.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.5.13)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (4.11.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (4.9.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.1.2)\n",
      "Requirement already satisfied: bleach in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (4.1.0)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbconvert->jupyter) (0.4)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\santi\\anaconda3\\lib\\site-packages (from notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from notebook->jupyter) (0.17.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from notebook->jupyter) (0.5.2)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\santi\\anaconda3\\lib\\site-packages (from notebook->jupyter) (0.14.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from qtconsole->jupyter) (2.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.18.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.4.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: stack-data in c:\\users\\santi\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter) (2.5.2)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-console->jupyter) (305.1)\n",
      "Requirement already satisfied: notebook-shim>=0.1.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter) (0.2.2)\n",
      "Requirement already satisfied: jupyter-server>=1.8 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbclassic>=0.4.7->notebook->jupyter) (1.23.4)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter) (4.17.3)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\santi\\anaconda3\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets->jupyter) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\santi\\anaconda3\\lib\\site-packages (from prompt-toolkit>=3.0.30->jupyter-console->jupyter) (0.2.5)\n",
      "Requirement already satisfied: pywinpty>=1.1.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from terminado>=0.8.3->notebook->jupyter) (2.0.10)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\santi\\anaconda3\\lib\\site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\santi\\anaconda3\\lib\\site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter) (22.1.0)\n",
      "Requirement already satisfied: websocket-client in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (0.58.0)\n",
      "Requirement already satisfied: anyio<4,>=3.1.0 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\santi\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\santi\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\santi\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->jupyter) (0.8.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\santi\\anaconda3\\lib\\site-packages (from anyio<4,>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter) (3.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\santi\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install jupyter pandas \"sqlalchemy<2.0\" pymysql seaborn pillow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión a la Base y Lectura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a la base...\n",
      "Consultando tabla con SQL: paragraphs\n",
      "Tiempo: 24.8 segundos\n",
      "Guardando: data\\shakespeare\\paragraphs.csv\n",
      "\n",
      "Consultando tabla con SQL: characters\n",
      "Tiempo: 1.2 segundos\n",
      "Guardando: data\\shakespeare\\characters.csv\n",
      "\n",
      "Consultando tabla con SQL: works\n",
      "Tiempo: 1.1 segundos\n",
      "Guardando: data\\shakespeare\\works.csv\n",
      "\n",
      "Consultando tabla con SQL: chapters\n",
      "Tiempo: 2.8 segundos\n",
      "Guardando: data\\shakespeare\\chapters.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"data\") / \"shakespeare\"\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_table(table_name, engine):\n",
    "    \"\"\"\n",
    "    Leer la tabla con SQL y guardarla como CSV,\n",
    "    o cargarla desde el CSV si ya existe\n",
    "    \"\"\"\n",
    "    path_table = data_dir / f\"{table_name}.csv\"\n",
    "    if not path_table.exists():\n",
    "        print(f\"Consultando tabla con SQL: {table_name}\")\n",
    "        t0 = time()\n",
    "        df_table = pd.read_sql(f\"SELECT * FROM {table_name}\", engine)\n",
    "        t1 = time()\n",
    "        print(f\"Tiempo: {t1 - t0:.1f} segundos\")\n",
    "\n",
    "        print(f\"Guardando: {path_table}\\n\")\n",
    "        df_table.to_csv(path_table)\n",
    "    else:\n",
    "        print(f\"Cargando tabla desde CSV: {path_table}\")\n",
    "        df_table = pd.read_csv(path_table, index_col=[0])\n",
    "    return df_table\n",
    "\n",
    "\n",
    "print(\"Conectando a la base...\")\n",
    "conn_str = \"mysql+pymysql://guest:relational@relational.fit.cvut.cz:3306/Shakespeare\"\n",
    "engine = create_engine(conn_str)\n",
    "\n",
    "# Todos los párrafos de todas las obras\n",
    "df_paragraphs = load_table(\"paragraphs\", engine)\n",
    "\n",
    "df_characters = load_table(\"characters\", engine)\n",
    "\n",
    "df_works = load_table(\"works\", engine)\n",
    "\n",
    "df_chapters = load_table(\"chapters\", engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paragraphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Actualizar con su versión de clean_text() de la Tarea_1\n",
    "\n",
    "def clean_text(df, column_name):\n",
    "    # Convertir todo a minúsculas\n",
    "    result = df[column_name].str.lower()\n",
    "\n",
    "    # FIXME: completar\n",
    "    for punc in [\"[\", \"\\n\", \",\"]:\n",
    "        result = result.str.replace(punc, \" \")\n",
    "    return result\n",
    "\n",
    "# Creamos una nueva columna CleanText a partir de PlainText\n",
    "df_paragraphs[\"CleanText\"] = clean_text(df_paragraphs, \"PlainText\")\n",
    "\n",
    "# Veamos la diferencia\n",
    "df_paragraphs[[\"PlainText\", \"CleanText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removemos el dato stage directions\n",
    "df_paragraphs = df_paragraphs[df_paragraphs[\"character_id\"]!=1261]\n",
    "#Removemos el dato Poet\n",
    "df_paragraphs = df_paragraphs[df_paragraphs[\"character_id\"]!=894]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos personajes, obras y géneros en el mismo dataset\n",
    "df_dataset = df_paragraphs.merge(df_chapters.set_index(\"id\")[\"work_id\"], left_on=\"chapter_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_works.set_index(\"id\")[[\"Title\", \"GenreType\"]], left_on=\"work_id\", right_index=True)\n",
    "df_dataset = df_dataset.merge(df_characters.set_index('id')[\"CharName\"], left_on=\"character_id\", right_index=True).sort_index()\n",
    "df_dataset = df_dataset[[\"CleanText\", \"CharName\", \"Title\", \"GenreType\"]]\n",
    "\n",
    "# Usaremos sólo estos personajes\n",
    "characters = [\"Antony\", \"Cleopatra\", \"Queen Margaret\"]\n",
    "df_dataset = df_dataset[df_dataset[\"CharName\"].isin(characters)]\n",
    "\n",
    "df_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Párrafos por cada personaje seleccionado\n",
    "df_dataset[\"CharName\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset y Features de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dataset[\"CleanText\"].to_numpy()\n",
    "y = df_dataset[\"CharName\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Partir train/test 30% estratificados\n",
    "# -> Definir X_train, X_test, y_train, y_test\n",
    "\n",
    "# X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "print(f\"Tamaños de Train/Test: {len(X_train)}/{len(X_test)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conteo de palabras y TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=None, ngram_range=(1,1))\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer(use_idf=False)\n",
    "X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "X_train_tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Realizar PCA sobre los datos de entrenamiento\n",
    "# reductor = ...\n",
    "\n",
    "# Transformar train\n",
    "X_train_red = reductor.fit_transform(X_train_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de las dos primeras componentes de PCA\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for character in np.unique(y_train):\n",
    "    mask_train = y_train == character\n",
    "    ax.scatter(X_train_red[mask_train, 0], X_train_red[mask_train, 1], label=character)\n",
    "\n",
    "ax.set_title(\"PCA por personaje\")\n",
    "ax.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bayes_clf = MultinomialNB().fit(X_train_tf, y_train)\n",
    "\n",
    "# Ver las primeras 10 predicciones de train\n",
    "y_pred_train = bayes_clf.predict(X_train_tf)\n",
    "y_pred_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_pred):\n",
    "    return (y_true == y_pred).sum() / len(y_true)\n",
    "\n",
    "get_accuracy(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Predecir para test y ver la matriz de confusión, y reportar accuracy\n",
    "\n",
    "# X_test_counts = ...\n",
    "# X_test_tfidf = ...\n",
    "# y_test_pred = ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Búsqueda de hiper-parámetros con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TODO: Agregar más variantes de parámetros que les parezcan relevantes\n",
    "param_sets = [{\"stop_words\": None, \"ngram\": (1,2), \"idf\": True},\n",
    "             {\"stop_words\": None, \"ngram\": (1,1), \"idf\": False}]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "# Ahora usaremos train/validation/test\n",
    "# Por lo tanto le renombramos train+validation = dev(elopment) dataset\n",
    "X_dev = X_train\n",
    "y_dev = y_train\n",
    "\n",
    "# # Para evitar errores\n",
    "# del X_train\n",
    "# del y_train\n",
    "\n",
    "for params in param_sets:\n",
    "    \n",
    "    # Transormaciones a aplicar (featurizers)\n",
    "    count_vect = CountVectorizer(stop_words=params[\"stop_words\"], ngram_range=params[\"ngram\"])\n",
    "    tf_idf = TfidfTransformer(use_idf=params[\"idf\"])\n",
    "    \n",
    "    for train_idxs, val_idxs in skf.split(X_dev, y_dev):\n",
    "        \n",
    "        # Train y validation para el split actual\n",
    "        X_train_ = X_dev[train_idxs]\n",
    "        y_train_ = y_dev[train_idxs]\n",
    "        X_val = X_dev[val_idxs]\n",
    "        y_val = y_dev[val_idxs]\n",
    "        \n",
    "        # Ajustamos y transformamos Train\n",
    "        X_train_counts = count_vect.fit_transform(X_train_)\n",
    "        X_train_tf = tf_idf.fit_transform(X_train_counts)\n",
    "        \n",
    "        # TODO: Completar el código para entrenar y evaluar \n",
    "        \n",
    "        # Entrenamos con Train\n",
    "        # bayes_clf = ...\n",
    "\n",
    "        # Transformamos Validation\n",
    "        # X_val_counts = ...\n",
    "        # X_val_tfidf = ...\n",
    "        \n",
    "        # Predecimos y evaluamos en Validation\n",
    "        y_pred_val = bayes_clf.predict(X_val_tfidf)\n",
    "        acc = get_accuracy(y_val, y_pred_val)\n",
    "        print(f\"{acc=:.4f} {params=}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Opcional) Comparativa con Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "y_train_s = np.char.replace(y_train.astype(str), \" \", \"_\").astype(object)\n",
    "y_test_s = np.char.replace(y_test.astype(str), \" \", \"_\").astype(object)\n",
    "\n",
    "# Convertimos al formato de fasttext: archivo de texto donde cada línea es:\n",
    "# __label__<label> TEXTO\n",
    "Xytrains = \"__label__\" + y_train_s.astype(object) + \" \" + X_train\n",
    "Xytests = \"__label__\" + y_test_s.astype(object) + \" \" + X_test\n",
    "np.savetxt(data_dir / \"train.txt\", Xytrains, fmt=\"%s\")\n",
    "np.savetxt(data_dir / \"test.txt\", Xytests, fmt=\"%s\")\n",
    "\n",
    "Xytests[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=str(data_dir / \"train.txt\"), epoch=100, wordNgrams=2)\n",
    "model.test(str(data_dir / \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_out = model.predict(list(X_test))\n",
    "y_pred_test = [y[0].replace(\"__label__\", \"\") for y in y_out[0]]\n",
    "    \n",
    "print(get_accuracy(y_test_s, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
